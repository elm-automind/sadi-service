add below as a tab on address static link page as communication channel and realtime exchange of messages: I want to implement a lightweight speech translation feature in my Replit project that uses as little compute power as possible while still being accurate. Please generate clean Python code (Flask or FastAPI) that: 1. Uses only Whisper Tiny or Whisper Base for speech-to-text (STT), running locally on CPU. - Do NOT use Whisper Medium or Large. - Make sure the model loads once at startup and is cached to avoid repeated loading. - Use openai-whisper or faster-whisper if lighter. 2. For translation, use a very small, cheap API model such as GPT-4o-mini or GPT-3.5-turbo. - The translation prompt should be optimized for short spoken sentences (English, Arabic, Urdu). - The translation step must NOT require heavy compute locally. 3. For text-to-speech (TTS), use either: - A very lightweight Coqui TTS model that works on CPU, OR - An API-based TTS call if it uses almost no local compute. - Avoid large TTS models like Bark or XTTS v2, since they require GPU. 4. Create a single endpoint called /voice-translate with: - An audio file upload - A target_language parameter ("en", "ar", "ur") 5. The endpoint should: - Run Whisper Tiny/Base locally to transcribe the audio. - Send the text to a translation API to get the translated text. - Convert that translated text to audio using Coqui TTS (small model) or TTS API. - Return JSON with the transcribed text + translated text, and return the audio file as a downloadable mp3. 6. Ensure all operations are extremely lightweight and optimized for Replitâ€™s CPU environment. - No GPU-only libraries - No huge model downloads - Avoid heavy NumPy or Torch operations except what's required by Whisper Tiny/Base. 7. Provide: - The full Python backend code - Requirements.txt - Code for pre-loading models efficiently - A sample JavaScript fetch() call using FormData to test the endpoint - CPU optimization tips for running Whisper Tiny/Base on Replit - Error handling for long audio files or slow inference The most important requirement is: **Keep compute load very low so that Whisper Tiny/Base runs fast and the project works smoothly on a $40 Replit plan.**